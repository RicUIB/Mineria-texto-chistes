---
title: "Exploración y curado de chistes"
author: ""
date: "19/2/2021"
output: 
  html_document: 
    toc: yes
    number_sections: yes
    df_print: kable
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache=TRUE)
library(tidyverse)
```

# Primera aproximacion de NLP para el análisis de un conjunto de chistes con título
##  Carga de datos


```{r}
data_raw=read_csv("tots.csv")
str(data_raw)
knitr::kable(head(data_raw,20))
colnames(data_raw)
text=data_raw$texto
tabla=table(unlist(lapply(text,FUN=function(x) Encoding(x))))
head(text)
head(tabla)
library(dplyr)
text_df <- tibble(line = 1:length(text), text_raw =text)%>% mutate(Enconding=Encoding(text_raw),text_utf8=enc2utf8(text))
```


## Extracción del diccionario raw empírico desde los chistes

Extraemos el dic_raw_1 todas las palabras que aparecen  con separación   espacio. 

Criterios iniciales:

* Decidimos enconding a UTF-8  columna  text_utf8 si hay que depurar por enconding habrá que ver cómo.
* Hay que decidir qué se hace con los CARACTERES SPECIALES:{,:; () ¿?!!}. De momento los voy a eliminar 
* Todas las MAYÚSCULAS a MINÚSCULAS
* De momento NO SE ELIMINAN DIGITOS: se quedan tal cual, hay que distinguir los de los dígitos de años.
* No catalogamos idiomas....  se supone que todo está en castellano o términos técnicos que añadiremos
* Castellano es toda palabra o   derivado de palabra que se encuentre en un spelling estándar de castellano que podemos ir adaptando.




```{r}
library(tidytext)
glimpse(text_df)
text_raw=text_df %>% unnest_tokens(word, text_utf8)
glimpse(text_raw)
knitr::kable(head(text_raw,20))
dic_raw_1=sort(unique(text_raw$word))
nw=length(dic_raw_1)# Hay 1627 palabras
nw
```

## Construcción del modelo de diccionario

Construiremos una tabla de modelado del corpus de palabras de los chistes:

* Como primary key la word ( las `nw` words) (desde el text_raw en utf8)
* Su frecuencia: número de veces que  aparece en los  diagnósticos
* Si es correcta  según un spelling de español  de España (hay que buscar... qué hay mejor)

```{r}
count_freq=text_raw %>% group_by(word) %>% summarise(N=n())

dic_raw_1 = tibble(word=dic_raw_1) %>% left_join(count_freq,by="word")
```


Ahora vemos claramente cómo podemos mejorar las words para UNIFICARLAS en un único "léxico" que nos permita un tratamiento unificado, auqnue las variantes escritas podrían tener significado humorístico.

Ejemplos


Palabras que contienen "zq"

```{r}
dic_raw_1[grep("zq",dic_raw_1$word),]
```


Palabras que  contienen "ch"

```{r}
dic_raw_1[grep("(ch)",dic_raw_1$word),]
```


Palabras (dos palabras) con :

```{r}
dic_raw_1[grep(":",dic_raw_1$word),]
```
### Añadimos columna  de spelling al diccionario


Primero veamos algunos ejemplos de las sugerencias: ver manual en de  [hunspell](https://docs.ropensci.org/hunspell/articles/intro.html).
[Github diccionarios open office](https://github.com/LibreOffice/dictionaries)

```{r}
library("spelling")
library("hunspell")
#https://github.com/titoBouzout/Dictionaries # do
#es=dictionary(lang = "diccionarios/es_ES.dic", affix = "diccionarios/es_ES.dic", add_words = NULL,  cache = FALSE)
es_ES<- dictionary("diccionarios/es_ES.dic")
#print(es_ES)
list_dictionaries()# estos son los que  vienen por defecto
hunspell_check(c("bieja","colon","colón"),dic= es_ES)
hunspell_suggest(c("bieja","colon","colón"),dic=es_ES)
palabras=c("amor", "amoroso", "amorosamente", "amado", "amante", "amador")
hunspell_analyze(palabras,dic=es_ES)
```

de momento tomaremos  sólo la primera sugerencia, aunque guardaremos todas.



```{r}
list_sugerences= sapply(dic_raw_1$word, FUN=function(x) hunspell_suggest(x,dic=es_ES))



dic_raw_1$list_sugerence_first=sapply(list_sugerences, FUN=function(x) x[1])
dic_raw_1$list_sugerence_all=sapply(list_sugerences,
                                          FUN=function(x){
                                            if(length(x)>=1) {return(paste(x,collapse=","))}
                                            if(length(x)==0){return(NA)}
                                            })
glimpse(dic_raw_1)
```


# Primer modelo de curado de los chistes



```{r}
knitr::kable(head(dic_raw_1,20))
```


### Salvar en excel 



```{r}
write_excel_csv2(x=dic_raw_1,file="dic_raw_1_2.csv")
```


```{r}
dic_raw_1_long_peticion = dic_raw_1 %>% right_join(text_raw,by="word")
write_excel_csv2(x=dic_raw_1_long_peticion,file="dic_raw_1_2_long_peticion.csv")
```


## Siguiente paso tratamineto de los datos curados y generación de las Document Term Matrix


Primera aproximación generación dela DTM del corpus de peticiones curadas. Cruzar estos datos con las prioridades.
Podéis hacerlo con tidytext o con tm (o con quanteda).

## Más chistes con metadatos

En el fichero de este git "chistes_con_metadatos.csv" hay más chistes con dos columnas de metadatos para practicar.
